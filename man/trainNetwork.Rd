% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/train.R
\name{trainNetwork}
\alias{trainNetwork}
\title{Trains a (mostly) Long short-term memory (LSTM) model on genomic data. Designed for developing genome based language models (GenomeNet)}
\usage{
trainNetwork(path, path.val, dataset, validation.split = 0.05,
  run.name = "run", maxlen = 250, dropout = 0,
  recurrent_dropout = 0, layer.size = 2048, batch.size = 512,
  layers.lstm = 4, solver = "adam", use.codon.cnn = FALSE,
  learning.rate = 0.001, use.cudnn = FALSE,
  use.multiple.gpus = FALSE, merge.on.cpu = TRUE, gpu.num = 2,
  vocabulary.size = 6, label.vocabulary.size = 3, epochs = 10,
  max.queue.size = 100, lr.plateau.factor = 0.9, patience = 5,
  cooldown = 5, steps.per.epoch = 10000, step = 1,
  randomFiles = FALSE, seqStart = "l", seqEnd = "l",
  withinFile = "p", vocabulary = c("l", "p", "a", "c", "g", "t"),
  tensorboard.log = "/scratch/tensorboard", bidirectional = FALSE)
}
\arguments{
\item{path}{Path to folder where individual or multiple FASTA files are located for training}

\item{path.val}{Path to folder where individual or multiple FASTA files are located for validation}

\item{dataset}{Dataframe holding training samples in RAM instead of using generator}

\item{validation.split}{Defines the fraction of the batches that will be used for validation}

\item{run.name}{Name of the run (without file ending)}

\item{maxlen}{Time steps to unroll for (e.g. length of semi-redundant chunks)}

\item{dropout}{Fraction of the units to drop for inputs}

\item{recurrent_dropout}{Fraction of the units to drop for recurrent state}

\item{layer.size}{Number of cells per network layer}

\item{batch.size}{Number of samples that are used for one network update}

\item{layers.lstm}{Number of LSTM layers}

\item{solver}{Optimizer on default adam (options: "adam", "adagrad", "rmsprop", "sgd")}

\item{use.codon.cnn}{First layer is a CNN layer with size of 3 to mimic codons (experimental)}

\item{learning.rate}{Learning rate for optimizer}

\item{use.cudnn}{If true, using layer_cudnn_lstm() instead of layer_lstm() which is if GPU supports cudnn}

\item{use.multiple.gpus}{If true, multi_gpu_model() will be used based on gpu_num}

\item{merge.on.cpu}{True on default, false recommend if the server supports NVlink, only relevant if use.multiple.gpu is true}

\item{gpu.num}{Number of GPUs to be used, only relevant if multiple_gpu is true}

\item{vocabulary.size}{Number of unique chars in training set}

\item{epochs}{Number of full iterations over the dataset}

\item{max.queue.size}{Queue on fit_generator()}

\item{lr.plateau.factor}{Factor of decreasing learning rate when plateau is reached}

\item{patience}{Number of epochs waiting for decrease in loss before reducing learningrate}

\item{cooldown}{Number of epochs without changing learningrate}

\item{steps.per.epoch}{Number of training samples divided by the batch.size, is 20139934 on SGB dataset}

\item{step}{frequency of sampling steps}

\item{randomFiles}{TRUE/FALSE go through files sequentially or shuffle beforehand}

\item{seqStart}{insert character at beginning of sequence}

\item{seqEnd}{insert character at end of sequence}

\item{withinFile}{insert characters within sequences}

\item{tensorboard.log}{path to tensorboard log directory}

\item{bidirectional}{Use bidirectional option for lstm layers}
}
\description{
Depth and number of neurons per layer of the netwok can be specified. First layer can be a Convolutional Neural Network (CNN) that is designed to capture codons.
If a path to a folder where FASTA files are located is provided, batches will ge generated using a external generator which
is recommended for big training sets. Alternative, a dataset can be supplied that holds the preprocessed batches (generated by \code{preprocessSemiRedundant()}) and keeps them in RAM. Supports also training on intances with multiple GPUs and scales linear with number of GPUs present.
}
