#' @title Trains a (mostly) Long short-term memory (LSTM) model on genomic data. Designed for developing genome based language models (GenomeNet)
#'
#' @description
#' Depth and number of neurons per layer of the netwok can be specified. First layer can be a Convolutional Neural Network (CNN) that is designed to capture codons.
#' If a path to a folder where FASTA files are located is provided, batches will ge generated using a external generator which
#' is recommended for big training sets. Alternative, a dataset can be supplied that holds the preprocessed batches (generated by \code{preprocessSemiRedundant()}) and keeps them in RAM. Supports also training on intances with multiple GPUs and scales linear with number of GPUs present.
#'
#' @param path Path to folder where individual or multiple FASTA files are located for training
#' @param path.val Path to folder where individual or multiple FASTA files are located for validation
#' @param dataset Dataframe holding training samples in RAM instead of using generator 
#' @param validation.split Defines the fraction of the batches that will be used for validation
#' @param run.name Name of the run (without file ending)
#' @param maxlen Time steps to unroll for (e.g. length of semi-redundant chunks)
#' @param dropout Fraction of the units to drop for inputs
#' @param recurrent_dropout Fraction of the units to drop for recurrent state
#' @param layer.size Number of cells per network layer
#' @param batch.size Number of samples that are used for one network update
#' @param layers.lstm Number of LSTM layers
#' @param solver Optimizer on default adam (options: "adam", "adagrad", "rmsprop", "sgd")
#' @param use.codon.cnn First layer is a CNN layer with size of 3 to mimic codons (experimental)
#' @param learning.rate Learning rate for optimizer
#' @param use.cudnn If true, using layer_cudnn_lstm() instead of layer_lstm() which is if GPU supports cudnn
#' @param use.multiple.gpus If true, multi_gpu_model() will be used based on gpu_num
#' @param gpu.num Number of GPUs to be used, only relevant if multiple_gpu is true
#' @param merge.on.cpu True on default, false recommend if the server supports NVlink, only relevant if use.multiple.gpu is true
#' @param vocabulary.size Number of unique chars in training set
#' @param epochs Number of full iterations over the dataset
#' @param max.queue.size Queue on fit_generator()
#' @param lr.plateau.factor Factor of decreasing learning rate when plateau is reached
#' @param patience Number of epochs waiting for decrease in loss before reducing learningrate
#' @param cooldown Number of epochs without changing learningrate
#' @param steps.per.epoch Number of training samples divided by the batch.size, is 20139934 on SGB dataset
#' @param tensorboard.log path to tensorboard log directory
#' @export
trainNetwork <- function(path,
                         path.val,
                         dataset,
                         validation.split = .05,
                         run.name = "run",
                         maxlen = 250,
                         dropout = 0,
                         recurrent_dropout = 0,
                         layer.size = 2048,
                         batch.size = 512,
                         layers.lstm = 4,
                         solver = "adam",
                         use.codon.cnn = FALSE,
                         learning.rate = .001,
                         use.cudnn = FALSE,
                         use.multiple.gpus = FALSE,
                         merge.on.cpu = TRUE,
                         gpu.num = 2,
                         vocabulary.size = 6,
                         label.vocabulary.size = 3,
                         epochs = 10,
                         max.queue.size = 100,
                         lr.plateau.factor = .1,
                         patience = 5,
                         cooldown = 5,
                         steps.per.epoch = 10000,
                         tensorboard.log = "/scratch/tensorboard") {
  
  stopifnot(maxlen > 0)
  stopifnot(dropout <= 1 & dropout >= 0)
  stopifnot(recurrent_dropout <= 1 & recurrent_dropout >= 0)
  stopifnot(layer.size > 1)
  stopifnot(layers.lstm > 1)
  stopifnot(batch.size > 1)
  stopifnot(steps.per.epoch > 0)
  
  if (dir.exists(file.path(tensorboard.log, run.name))) {
    stop(paste0("Tensorboard entry '", run.name , "' is already present. Please give your run a unique name."))
  }
  
  if (use.cudnn & (recurrent_dropout > 0 | recurrent_dropout > 0)){
    warning("Dropout is not supported by cuDNN and will be ignored")
  } 
  
  message("Initialize model. This can take a few minutes.")
  if (use.multiple.gpus) {
    # init template model under a CPU device scope
    with(tf$device("/cpu:0"), {
      model <- keras::keras_model_sequential()
    })
  } else {
    model <- keras::keras_model_sequential()
  }
  
  if (use.codon.cnn) {
    model %<>%
      keras::layer_conv_1d(
        kernel_size = 3,
        # 3 charactes are representing a codon
        padding = "same",
        activation = "relu",
        filters = 81,
        input_shape = c(maxlen, vocabulary.size)
      )  %>%
      keras::layer_max_pooling_1d(pool_size = 3)  %>%
      keras::layer_batch_normalization(momentum = .8)
  }
  
  # following layers
  if (use.cudnn) {
    for (i in 1:(layers.lstm - 1)) {
      model %>%
        keras::layer_cudnn_lstm(
          layer.size,
          input_shape = c(maxlen, vocabulary.size),
          return_sequences = T
        ) 
    }
    # last LSTM layer
    model %>%
      keras::layer_cudnn_lstm(layer.size) 
  } else {
    # non-cudnn
    for (i in 1:(layers.lstm - 1)) {
      model %>%
        keras::layer_lstm(
          layer.size,
          input_shape = c(maxlen, vocabulary.size),
          return_sequences = T,
          dropout = dropout,
          recurrent_dropout = recurrent_dropout
        )
    }
    # last LSTM layer
    model %>%
      keras::layer_lstm(layer.size, dropout = dropout, recurrent_dropout = recurrent_dropout)
  }
  
  model %>% keras::layer_dense(vocabulary.size) %>%
    keras::layer_activation("softmax")
  
  # print model layout to screen, should be done before multi_gpu_model
  summary(model)
  
  if (use.multiple.gpus) {
    model <- keras::multi_gpu_model(model,
                                    gpus = gpu.num,
                                    cpu_merge = merge.on.cpu)
  }
  
  # choose optimization method
  if (solver == "adam")
    optimizer <-
    keras::optimizer_adam(lr = learning.rate)
  if (solver == "adagrad")
    optimizer <-
    keras::optimizer_adagrad(lr = learning.rate)
  if (solver == "rmsprop")
    optimizer <-
    keras::optimizer_rmsprop(lr = learning.rate)
  if (solver == "sgd")
    optimizer <-
    keras::optimizer_sgd(lr = learning.rate)
  
  # start TB file writer 
  file.writer <- tensorflow::tf$summary$create_file_writer(file.path(tensorboard.log, run.name))
  file.writer$set_as_default()
  
  model %>% keras::compile(loss = "categorical_crossentropy",
                           optimizer = optimizer, metrics = c("acc"))
  
  # if no dataset is supplied, external fasta generator will generate batches
  if (missing(dataset)) {
    message("Starting fasta generator ...")
    # generator for training
    gen <-
      fastaFileGenerator(corpus.dir = path, batch.size = batch.size, maxlen = maxlen)
    # generator for validation
    gen.val <-
      fastaFileGenerator(corpus.dir = path.val, batch.size = batch.size, maxlen = maxlen)
    
    # training
    message("Start training ...")
    history <-
      model %>% keras::fit_generator(
        generator = gen,
        validation_data = gen.val,
        validation_steps = floor(steps.per.epoch/20),
        steps_per_epoch = steps.per.epoch,
        max_queue_size = max.queue.size,
        epochs = epochs,
        callbacks = list(
          keras::callback_model_checkpoint(paste0(run.name, "_checkpoints.h5")),
          keras::callback_reduce_lr_on_plateau(
            monitor = "loss",
            factor = lr.plateau.factor,
            patience = patience,
            cooldown = cooldown
          ),
          ecoliCustomScalar(model, vocabulary= vocabulary, maxlen = maxlen),
          keras::callback_tensorboard(file.path(tensorboard.log, run.name),
                                      write_graph = T, 
                                      histogram_freq = 1,
                                      write_images = T,
                                      write_grads = T
          ),
          keras::callback_csv_logger(
            paste0(run.name, "_log.csv"),
            separator = ";",
            append = TRUE)
        )
      )
  } else {
    message("Start training ...")
    history <- model %>% keras::fit(
      dataset$X,
      dataset$Y,
      batch_size = batch.size,
      validation_split = validation.split,
      epochs = epochs
    )
  }
  
  # save final model
  message("Training done.\nSave model.")
  Rmodel <-
    keras::serialize_model(model, include_optimizer = TRUE)
  save(Rmodel, file = paste0(run.name, "_full_model.Rdata"))
  keras::save_model_hdf5(
    model,
    paste0(run.name, "_full_model.hdf5"),
    overwrite = TRUE,
    include_optimizer = TRUE
  )
  return(history)
}
