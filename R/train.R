#' @title Trains a (mostly) Long short-term memory (LSTM) model on genomic data. Designed for developing genome based language models (GenomeNet)
#'
#' @description
#' Depth and number of neurons per layer of the netwok can be specified. First layer can be a Convolutional Neural Network (CNN) that is designed to capture codons.
#' If a path to a folder where FASTA files are located is provided, batches will ge generated using a external generator which
#' is recommended for big training sets. Alternative, a dataset can be supplied that holds the preprocessed batches (generated by \code{preprocessSemiRedundant()}) and keeps them in RAM. Supports also training on intances with multiple GPUs and scales linear with number of GPUs present.
#' @param model_path Path to a pretrained model
#' @param path Path to folder where individual or multiple FASTA files are located for training
#' @param path.val Path to folder where individual or multiple FASTA files are located for validation
#' @param dataset Dataframe holding training samples in RAM instead of using generator 
#' @param checkpoint_path Path to checkpoints folder 
#' @param validation.split Defines the fraction of the batches that will be used for validation
#' @param run.name Name of the run (without file ending)
#' @param maxlen Time steps to unroll for (e.g. length of semi-redundant chunks)
#' @param dropout Fraction of the units to drop for inputs
#' @param recurrent_dropout Fraction of the units to drop for recurrent state
#' @param layer.size Number of cells per network layer
#' @param batch.size Number of samples that are used for one network update
#' @param layers.lstm Number of LSTM layers
#' @param solver Optimizer on default adam (options: "adam", "adagrad", "rmsprop", "sgd")
#' @param use.codon.cnn First layer is a CNN layer with size of 3 to mimic codons (experimental)
#' @param learning.rate Learning rate for optimizer
#' @param use.cudnn If true, using layer_cudnn_lstm() instead of layer_lstm() which is if GPU supports cudnn
#' @param use.multiple.gpus If true, multi_gpu_model() will be used based on gpu_num
#' @param gpu.num Number of GPUs to be used, only relevant if multiple_gpu is true
#' @param merge.on.cpu True on default, false recommend if the server supports NVlink, only relevant if use.multiple.gpu is true
#' @param vocabulary.size Number of unique chars in training set
#' @param epochs Number of full iterations over the dataset
#' @param max.queue.size Queue on fit_generator()
#' @param lr.plateau.factor Factor of decreasing learning rate when plateau is reached
#' @param patience Number of epochs waiting for decrease in loss before reducing learningrate
#' @param cooldown Number of epochs without changing learningrate
#' @param steps.per.epoch Number of training samples divided by the batch.size, is 20139934 on SGB dataset
#' @param step Frequency of sampling steps
#' @param randomFiles TRUE/FALSE go through files sequentially or shuffle beforehand
#' @param seqStart Insert character at beginning of sequence
#' @param seqEnd Insert character at end of sequence
#' @param withinFile Insert characters within sequences
#' @param initial_epoch Epoch at which to start training, set to 0 if no model_path given
#' @param vocabulary Vector of allowed characters, samples with other chars get discarded
#' @param tensorboard.log Path to tensorboard log directory
#' @param bidirectional Use bidirectional option for lstm layers
#' @param save_best_only Only save model that improved on best val_loss score 
#' @param compile Whether to compile the model after loading
#' @export
trainNetwork <- function(model_path,
                         path,
                         path.val,
                         dataset,
                         checkpoint_path = "/scratch/checkpoints", 
                         validation.split = .05,
                         run.name = "run",
                         maxlen,
                         dropout = 0,
                         recurrent_dropout = 0,
                         layer.size = 2048,
                         batch.size = 512,
                         layers.lstm = 4,
                         solver = "adam",
                         use.codon.cnn = FALSE,
                         learning.rate = .001,
                         use.cudnn = FALSE,
                         use.multiple.gpus = FALSE,
                         merge.on.cpu = TRUE,
                         gpu.num = 2,
                         vocabulary.size = 6,
                         label.vocabulary.size = 3,
                         epochs = 10,
                         max.queue.size = 100,
                         lr.plateau.factor = 0.9,
                         patience = 5,
                         cooldown = 5,
                         steps.per.epoch = 10000,
                         step = 1,
                         randomFiles = FALSE,
                         seqStart = "l",
                         seqEnd= "l",
                         withinFile = "p",
                         initial_epoch,
                         vocabulary = c("l", "p", "a", "c", "g", "t"),
                         tensorboard.log = "/scratch/tensorboard",
                         bidirectional = FALSE,
                         save_best_only = FALSE,
                         compile = TRUE) {  
  
  ## create folder for checkpoints using run.name
  ## filenames contain epoch, validation loss and validation accuracy 
  checkpoint_dir <- paste0(checkpoint_path, "/", run.name, "_checkpoints")
  dir.create(checkpoint_dir, showWarnings = FALSE)
  filepath_checkpoints <- file.path(checkpoint_dir, "Ep.{epoch:03d}-val_loss{val_loss:.2f}-val_acc{val_acc:.3f}.hdf5")
  
  # Check if run.name is unique
  if (dir.exists(file.path(tensorboard.log, run.name))) {
    stop(paste0("Tensorboard entry '", run.name , "' is already present. Please give your run a unique name."))
  }
  
  # No pretrained model given
  if (missing(model_path)){ 
    
    initial_epoch <- 0
    
    stopifnot(maxlen > 0)
    stopifnot(dropout <= 1 & dropout >= 0)
    stopifnot(recurrent_dropout <= 1 & recurrent_dropout >= 0)
    stopifnot(layer.size > 1)
    stopifnot(layers.lstm > 1)
    stopifnot(batch.size > 1)
    stopifnot(steps.per.epoch > 0)
    
    if (use.cudnn & (recurrent_dropout > 0 | recurrent_dropout > 0)){
      warning("Dropout is not supported by cuDNN and will be ignored")
    } 
    
    message("Initialize model. This can take a few minutes.")
    if (use.multiple.gpus) {
      # init template model under a CPU device scope
      with(tf$device("/cpu:0"), {
        model <- keras::keras_model_sequential()
      })
    } else {
      model <- keras::keras_model_sequential()
    }
    
    if (use.codon.cnn) {
      model %<>%
        keras::layer_conv_1d(
          kernel_size = 3,
          # 3 charactes are representing a codon
          padding = "same",
          activation = "relu",
          filters = 81,
          input_shape = c(maxlen, vocabulary.size)
        )  %>%
        keras::layer_max_pooling_1d(pool_size = 3)  %>%
        keras::layer_batch_normalization(momentum = .8)
    }
    
    # following layers
    if (use.cudnn) {
      if (bidirectional){
        for (i in 1:(layers.lstm - 1)) {
          model %>%
            keras::bidirectional(
              input_shape = c(maxlen, vocabulary.size),
              keras::layer_cudnn_lstm(
                units = layer.size,
                return_sequences = TRUE
              ) 
            )
        } 
        
      } else {
        for (i in 1:(layers.lstm - 1)) {
          model %>%
            keras::layer_cudnn_lstm(
              layer.size,
              input_shape = c(maxlen, vocabulary.size),
              return_sequences = TRUE
            )
        } 
      }
      # last LSTM layer
      if (bidirectional){
        model %>%
          keras::bidirectional(
            keras::layer_cudnn_lstm(units = layer.size)
          )
      } else {
        model %>% keras::layer_cudnn_lstm(layer.size)
      }
      
    } else {
      # non-cudnn
      if (bidirectional){
        for (i in 1:(layers.lstm - 1)) {
          model %>%
            keras::bidirectional(
              input_shape = c(maxlen, vocabulary.size),
              keras::layer_lstm(
                units = layer.size,
                return_sequences = TRUE,
                dropout = dropout,
                recurrent_dropout = recurrent_dropout
              )
            )
        } 
      } else {
        for (i in 1:(layers.lstm - 1)) {
          model %>%
            keras::layer_lstm(
              layer.size,
              input_shape = c(maxlen, vocabulary.size),
              return_sequences = TRUE
              
            )
        } 
      }
      # last LSTM layer
      if (bidirectional){
        model %>%
          keras::bidirectional(
            keras::layer_lstm(units = layer.size, dropout = dropout, recurrent_dropout = recurrent_dropout)
          )
      } else {
        model %>%
          keras::layer_lstm(layer.size, dropout = dropout, recurrent_dropout = recurrent_dropout)
      }
    }
    
    model %>% keras::layer_dense(vocabulary.size) %>%
      keras::layer_activation("softmax")
    
    # print model layout to screen, should be done before multi_gpu_model 
    summary(model)
    
    if (use.multiple.gpus) {
      model <- keras::multi_gpu_model(model,
                                      gpus = gpu.num,
                                      cpu_merge = merge.on.cpu)
    }
    
    # choose optimization method
    if (solver == "adam")
      optimizer <-
      keras::optimizer_adam(lr = learning.rate)
    if (solver == "adagrad")
      optimizer <-
      keras::optimizer_adagrad(lr = learning.rate)
    if (solver == "rmsprop")
      optimizer <-
      keras::optimizer_rmsprop(lr = learning.rate)
    if (solver == "sgd")
      optimizer <-
      keras::optimizer_sgd(lr = learning.rate)
    
    model %>% keras::compile(loss = "categorical_crossentropy",
                             optimizer = optimizer, metrics = c("acc"))
    
    # Use pretrained model     
  } else { 
    
    # message for ignored arguments
    if (!missing(maxlen) | !missing(dropout) | !missing(recurrent_dropout) | !missing(layer.size) | !missing(use.cudnn) |
        !missing(layers.lstm) | !missing(use.codon.cnn) | !missing(vocabulary.size) | !missing(bidirectional)){
      message("The following parameters are predetermined by the loaded model (duplicate arguments in function will be ignored): 
              maxlen, dropout, recurrent_dropout, layer.size, use.cudnn, layers.lstm, use.codon.cnn, vocabulary.size, bidirectional")
    }
   
    # epochs arguments can be misleading 
    if (!missing(initial_epoch)){
      if (initial_epoch > epochs){
        stop("Networks trains (epochs - initial_epochs) rounds overall, NOT epochs rounds. Increase epochs or decrease initial_epoch.")
      }
    }
    
    # extract initial_epoch from filename if no argument is given
    if (missing(initial_epoch)){
      epochFromFilename <- stringr::str_extract(model_path, "Ep.\\d+")
      initial_epoch <- as.integer(substring(epochFromFilename, 4, nchar(epochFromFilename)))
      if (initial_epoch > epochs){
        stop("Networks trains (epochs - initial_epochs) rounds overall, NOT epochs rounds. Increase epochs or decrease initial_epoch.")
      }
    }
    
    # load model
    model <- keras::load_model_hdf5(model_path, compile = compile)
    summary(model)
    
    # extract maxlen
    maxlen <- model$input$shape[1] 
    
    if (compile & (!missing(learning.rate)|!missing(solver))){
      message("Arguments for solver and learning rate will be ignored. Set compile to FALSE to use costum solver and learning rate.")
    }
    
    if (!compile){
      # choose optimization method
      if (solver == "adam")
        optimizer <-
          keras::optimizer_adam(lr = learning.rate)
      if (solver == "adagrad")
        optimizer <-
          keras::optimizer_adagrad(lr = learning.rate)
      if (solver == "rmsprop")
        optimizer <-
          keras::optimizer_rmsprop(lr = learning.rate)
      if (solver == "sgd")
        optimizer <-
          keras::optimizer_sgd(lr = learning.rate) 
      
      model %>% keras::compile(loss = "categorical_crossentropy",
                               optimizer = optimizer, metrics = c("acc"))
    }
    
  }
  
  # if no dataset is supplied, external fasta generator will generate batches
  if (missing(dataset)) {
    message("Starting fasta generator...")
    # generator for training
    gen <- fastaFileGenerator(corpus.dir = path, batch.size = batch.size,
                              maxlen = maxlen, step = step, randomFiles = randomFiles,
                              seqStart = seqStart, seqEnd= seqEnd, withinFile = withinFile,
                              vocabulary = vocabulary)
    
    # generator for validation
    gen.val <- fastaFileGenerator(corpus.dir = path.val, batch.size = batch.size,
                                  maxlen = maxlen, step = step, randomFiles = randomFiles,
                                  seqStart = seqStart, seqEnd= seqEnd, withinFile = withinFile,
                                  vocabulary = vocabulary)
    
    # training
    message("Start training ...")
    history <-
      model %>% keras::fit_generator(
        generator = gen,
        validation_data = gen.val,
        validation_steps = ceiling(steps.per.epoch * validation.split),
        steps_per_epoch = steps.per.epoch,
        max_queue_size = max.queue.size,
        epochs = epochs,
        initial_epoch = initial_epoch,
        callbacks = list(
          keras::callback_model_checkpoint(filepath = filepath_checkpoints,
                                           save_weights_only = FALSE,
                                           save_best_only = save_best_only,
                                           verbose = 1),
          
          keras::callback_reduce_lr_on_plateau(
            monitor = "loss",
            factor = lr.plateau.factor,
            patience = patience,
            cooldown = cooldown
          ),
          keras::callback_tensorboard(file.path(tensorboard.log, run.name),
                                      write_graph = TRUE, 
                                      histogram_freq = 1,
                                      write_images = TRUE,
                                      write_grads = TRUE),
          keras::callback_csv_logger(
            paste0(run.name, "_log.csv"),
            separator = ";",
            append = TRUE)
        )
      )
  } else {
    message("Start training ...")
    history <- model %>% keras::fit(
      dataset$X,
      dataset$Y,
      batch_size = batch.size,
      validation_split = validation.split,
      epochs = epochs)
    
  }
  
  # save final model
  message("Training done.\nSave model.")
  Rmodel <-
    keras::serialize_model(model, include_optimizer = TRUE)
  save(Rmodel, file = paste0(run.name, "_full_model.Rdata"))
  keras::save_model_hdf5(
    model,
    paste0(run.name, "_full_model.hdf5"),
    overwrite = TRUE,
    include_optimizer = TRUE
  )
  return(history)
}
