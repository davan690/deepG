#' @title Trains a (mostly) LSTM model on genomic data. Designed for developing genome based language models (GenomeNet)
#'
#' @description
#' Depth and number of neurons per layer of the netwok can be specified. First layer can be a CNN that is designed to capture codons.
#' If a path to a folder where fasta files are located is provided, batches will ge generated using a external generator which
#' is recommended for big training sets. Alternative, a dataset can be supplied that holds the preprocessed batches (generated by \code{preprocessSemiRedundant}) and their
#' labels and keeps them in RAM. Supports also training on intances with multiple GPUs and scales linear with number of GPUs present.
#'
#' @param path Path to folder where individual or multiple fasta files are located
#' @param dataset Dataframe holding training samples in RAM instead of using generator. 
#' @param labels path to folder where labels are located
#' @param validation.split Defines the fraction of the batches that will be used for validation.
#' @param run.name name of the run (without file ending)
#' @param maxlen Time steps to unroll for (e.g. length of semi-redundant chunks).
#' @param dropout.rate Dropout rate for LSTM cells
#' @param layer.size Number of cells per network layer
#' @param batch.size Number of samples that are used for one network update
#' @param layers.lstm Number of LSTM layers
#' @param solver optimizer (on default Adam)
#' @param use.codon.cnn first layer is a CNN layer with size of 3 to mimic codons (experimental)
#' @param learning.rate learning rate for optimizer
#' @param layer.size number of cells per network layer#'
#' @param use.cudnn if true, using layer_cudnn_lstm() instead of layer_lstm() which is if GPU supports cudnn
#' @param use.multiple.gpus if true, multi_gpu_model() will be used based on gpu_num
#' @param gpu.num number of GPUs to be used, only relevant if multiple_gpu is true
#' @param merge.on.cpu true on default, false recommend if the server supports NVlink, only relevant if use.multiple.gpu is true
#' @param vocabulary.size number of unique chars in training set'
#' @param epochs number of full iterations over the dataset
#' @param max.queue.size queue on fit_generator()
#' @param lr.plateau.factor factor of decreasing learning rate when plateau ich reached
#' @param patience number of epochs wating for decrease in loss before reducing learing rate
#' @param cooldown number of epochs without changing learning rate
#' @param steps.per.epoch number of training samples divided by the batch_size, is 20139934 on SGB dataset
#' @export
trainNetwork <- function(path,
                         dataset,
                         labels,
                         validation.split = .05,
                         run.name = "run",
                         maxlen = 250,
                         dropout.rate = .3,
                         layer.size = 2048,
                         batch.size = 512,
                         layers.lstm = 4,
                         solver = "adam",
                         use.codon.cnn = FALSE,
                         learning.rate = .001,
                         use.cudnn = FALSE,
                         use.multiple.gpus = FALSE,
                         merge.on.cpu = TRUE,
                         gpu.num = 2,
                         vocabulary.size = 6,
                         label.vocabulary.size = 3,
                         epochs = 10,
                         max.queue.size = 100,
                         lr.plateau.factor = .1,
                         patience = 5,
                         cooldown = 5,
                         steps.per.epoch = "auto") {
  library(dplyr)
  library(keras)
  library(magrittr)
  library(tensorflow)
  
  Check <- ArgumentCheck::newArgCheck()
  #* Add an error if maxlen <1
  if (maxlen < 1)
    ArgumentCheck::addError(msg = "'maxlen' must be >= 1",
                            argcheck = Check)
  #* Add an error if dropout_rate is < 0 or > 1
  if (dropout.rate > 1 |  dropout.rate < 0)
    ArgumentCheck::addError(msg = "'dropout.rate' must be between 0 and 1",
                            argcheck = Check)
  #* Add an error if layer_size is smaller than 1
  if (layer.size < 1)
    ArgumentCheck::addError(msg = "'layer.size' should be a positive integer",
                            argcheck = Check)
  #* Add an error if number of layers is smaller than 1
  if (layers.lstm < 1)
    ArgumentCheck::addError(msg = "'layers.lstm' should be a positive integer",
                            argcheck = Check)
  #* Add an error if layer_size negative
  if (batch.size < 1)
    ArgumentCheck::addError(msg = "'batch.size' should be a positive integer",
                            argcheck = Check)
  #* Return errors and warnings (if any)
  ArgumentCheck::finishArgCheck(Check)
  
  messagef("Initialize model. This can take a few minutes.")
  if (use.multiple.gpus) {
    # init template model under a CPU device scope
    with(tf$device("/cpu:0"), {
      model <- keras::keras_model_sequential()
    })
  } else {
    model <- keras::keras_model_sequential()
  }
  
  if (use.codon.cnn) {
    model %<>%
      keras::layer_conv_1d(
        kernel_size = 3,
        # 3 charactes are representing a codon
        padding = "same",
        activation = "relu",
        filters = 81,
        input_shape = c(maxlen, vocabulary.size)
      )  %>%
      layer_max_pooling_1d(pool_size = 3)  %>%
      layer_batch_normalization(momentum = .8)
  }
  
  # following layers
  if (use.cudnn) {
    for (i in 1:(layers.lstm - 1)) {
      model %>%
        keras::layer_cudnn_lstm(
          layer.size,
          input_shape = c(maxlen, vocabulary.size),
          return_sequences = T
        ) %>%
        keras::layer_dropout(rate = dropout.rate)
    }
    # last LSTM layer
    model %>%
      keras::layer_cudnn_lstm(layer.size) %>%
      keras::layer_dropout(rate = dropout.rate)
  } else {
    # non-cudnn
    for (i in 1:(layers.lstm - 1)) {
      model %<>%
        keras::layer_lstm(
          layer.size,
          input_shape = c(maxlen, vocabulary.size),
          return_sequences = T
        ) %>%
        keras::layer_dropout(rate = dropout.rate)
    }
    # last LSTM layer
    model %>%
      keras::layer_lstm(layer.size) %>%
      keras::layer_dropout(rate = dropout.rate)
  }
  
  if (missing(labels)) {
    # last dense layer
    model %>% keras::layer_dense(vocabulary.size) %>%
      keras::layer_activation("softmax")
  } else {
    # last dense layer with label vocabulary
    model %>% keras::layer_dense(label.vocabulary.size) %>%
      keras::layer_activation("softmax")
  } 
  
  # print model layout to screen, should be done before multi_gpu_model
  summary(model)
  
  if (use.multiple.gpus) {
    model <- keras::multi_gpu_model(model,
                                    gpus = gpu.num,
                                    cpu_merge = merge.on.cpu)
  }
  
  # choose optimization method
  if (solver == "adam")
    optimizer <-
    keras::optimizer_adam(lr = learning.rate)
  if (solver == "adagrad")
    optimizer <-
    keras::optimizer_adagrad(lr = learning.rate)
  if (solver == "rmsprop")
    optimizer <-
    keras::optimizer_rmsprop(lr = learning.rate)
  if (solver == "sgd")
    optimizer <-
    keras::optimizer_sgd(lr = learning.rate)
  
  model %>% keras::compile(loss = "categorical_crossentropy",
                           optimizer = optimizer)
  
  # if no dataset is supplied, external fasta generator will generate batches
  if (missing(dataset)) {
    messagef("Starting fasta generator.")
    if (missing(labels)) {
      gen <-
        fastaFileGenerator(corpus.dir = path, batch.size = batch.size, maxlen = maxlen)
    } else {
      gen <- fastaFileGenerator(corpus.dir = path, labels.dir= labels, batch.size = batch.size, maxlen = maxlen)
      
    }
    
    
    # calculate the number of steps after one epoch is finished (full iteration)
    if (steps.per.epoch == "auto") {
      messagef("Calculating required steps per epoch.")
      steps.per.epoch <-
        calculateStepsPerEpoch(path, maxlen = maxlen, batch.size = batch.size)
      messagef(paste("Full epoch requires", steps.per.epoch, "steps."))
    }
    # training
    messagef("Start training.")
    history <-
      model %>% keras::fit_generator(
        generator = gen,
        steps_per_epoch = steps.per.epoch,
        max_queue_size = max.queue.size,
        epochs = epochs,
        callbacks = list(
          callback_model_checkpoint(paste0(run.name, "_checkpoints.h5"), period = 5),
          callback_reduce_lr_on_plateau(
            monitor = "loss",
            factor = lr.plateau.factor,
            patience = patience,
            cooldown = cooldown
          ),
          callback_csv_logger(
            paste0(run.name, "_log.csv"),
            separator = ";",
            append = TRUE
          )
        )
      )
  } else {
    # dataframe is given
    messagef("Start training.")
    history <- model %>% keras::fit(
      dataset$X,
      dataset$Y,
      batch_size = batch.size,
      validation_split = validation.split,
      epochs = epochs
    )
  }
  
  # save final model
  messagef("Training done.\nSave model.")
  Rmodel <-
    keras::serialize_model(model, include_optimizer = TRUE)
  save(Rmodel, file = paste0(run.name, "_full_model.Rdata"))
  keras::save_model_hdf5(
    model,
    paste0(run.name, "_full_model.hdf5"),
    overwrite = TRUE,
    include_optimizer = TRUE
  )
  return(history)
}

